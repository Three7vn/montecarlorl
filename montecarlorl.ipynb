{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d170e6b9-bcdd-4cda-987b-73a138862086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "792a752e-caa3-4fb7-bc51-14f6a1a34bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the blackjack environment\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b32de90c-f1c5-49a9-94d1-57cbaa59044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "#display observation and action space\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Action space: {env.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41e8bab0-34c9-45ea-83e1-c9cb99f23b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a custom policy for decision-making\n",
    "def custom_policy(state):\n",
    "    #fetch the player's hand value\n",
    "    player_total = state[0]\n",
    "\n",
    "    #decide to stand (0) if total is 19 or above, otherwise request card (1)\n",
    "    if player_total >= 19:\n",
    "        return 0  #stand\n",
    "    else:\n",
    "        return 1  #request card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b82597fb-3c1f-4a3a-a65d-67b154678466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to play multiple games\n",
    "def play_blackjack(env, policy, games=5):\n",
    "    results = []  #store results of all games\n",
    "    for game in range(games):\n",
    "        #reset the game\n",
    "        state = env.reset()\n",
    "        game_log = []  #log states, actions, and rewards for this game\n",
    "\n",
    "        print(f'Game {game + 1} starts!')\n",
    "\n",
    "        while True:\n",
    "            action = policy(state)  #decide the action based on policy\n",
    "            next_state, reward, done, _ = env.step(action)  #take a step in the environment\n",
    "            game_log.append((state, action, reward))  #log the step\n",
    "            state = next_state  #update to the new state\n",
    "\n",
    "            #check if the game is over\n",
    "            if done:\n",
    "                print(f'Game {game + 1} ends! Outcome: {reward}')\n",
    "                results.append({'reward': reward, 'log': game_log})  #save the game outcome\n",
    "                break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1652b18d-0c72-4441-a876-dc5feb382da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monte carlo prediction to estimate the action-value function\n",
    "def monte_carlo_prediction(env, policy, episodes, gamma=1.0):\n",
    "    #initialise dictionaries to track returns and counts\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    #iterate over multiple episodes\n",
    "    for episode_idx in range(1, episodes + 1):\n",
    "        if episode_idx % 500 == 0:\n",
    "            print(f'Episode {episode_idx}/{episodes}')\n",
    "\n",
    "        #generate a single episode\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        while True:\n",
    "            action = policy(state)  #select action based on policy\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))  #log the episode\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #process the episode for returns and Q updates\n",
    "        visited = set()\n",
    "        for idx, (state, action, reward) in enumerate(episode):\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                #calculate the return\n",
    "                G = sum(gamma**i * episode[i][2] for i in range(idx, len(episode)))\n",
    "                returns_sum[state][action] += G  #accumulate returns\n",
    "                N[state][action] += 1  #increment visit count\n",
    "                Q[state][action] = returns_sum[state][action] / N[state][action]  #update Q\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "476b8ece-fc65-415d-be63-63c7a7291ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monte carlo control to optimise the policy\n",
    "def monte_carlo_control(env, episodes, alpha=0.1, gamma=1.0):\n",
    "    #initialise Q-values and policy\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    policy = defaultdict(int)\n",
    "\n",
    "    #iterate through episodes\n",
    "    for episode_idx in range(1, episodes + 1):\n",
    "        if episode_idx % 500 == 0:\n",
    "            print(f'Episode {episode_idx}/{episodes}')\n",
    "\n",
    "        #generate an episode with the current policy\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        while True:\n",
    "            #use epsilon-greedy policy to encourage exploration\n",
    "            epsilon = 1.0 / (episode_idx + 1)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #process the episode for constant-alpha updates\n",
    "        for idx, (state, action, reward) in enumerate(episode):\n",
    "            #calculate return\n",
    "            G = sum(gamma**i * episode[i][2] for i in range(idx, len(episode)))\n",
    "            #update Q-value with constant-alpha rule\n",
    "            Q[state][action] += alpha * (G - Q[state][action])\n",
    "\n",
    "        #update policy to be greedy with respect to Q\n",
    "        for state in Q.keys():\n",
    "            policy[state] = np.argmax(Q[state])\n",
    "\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e21c2c1d-ee8d-415a-8a70-8eebf78411b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to analyse results\n",
    "def analyse_results(results):\n",
    "    total_rewards = sum(game['reward'] for game in results)\n",
    "    wins = sum(1 for game in results if game['reward'] > 0)\n",
    "    losses = sum(1 for game in results if game['reward'] < 0)\n",
    "    draws = len(results) - wins - losses\n",
    "\n",
    "    #print summary statistics\n",
    "    print('\\n --Game Analysis--')\n",
    "    print(f'Total rewards: {total_rewards}')\n",
    "    print(f'Wins: {wins}, Losses: {losses}, Draws: {draws}')\n",
    "\n",
    "    #plot outcome distribution\n",
    "    outcomes = [game['reward'] for game in results]\n",
    "    plt.hist(outcomes, bins=np.arange(-1.5, 2, 1), edgecolor='black')\n",
    "    plt.title('Distribution of Game Outcomes')\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bff15762-5561-4075-b24f-a608c13ceca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 starts!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#play 5 games using the custom policy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m play_blackjack(env, custom_policy, games\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#analyse the game results\u001b[39;00m\n\u001b[1;32m      5\u001b[0m analyse_results(results)\n",
      "Cell \u001b[0;32mIn[79], line 12\u001b[0m, in \u001b[0;36mplay_blackjack\u001b[0;34m(env, policy, games)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m starts!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy(state)  \u001b[38;5;66;03m#decide the action based on policy\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m#take a step in the environment\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     game_log\u001b[38;5;241m.\u001b[39mappend((state, action, reward))  \u001b[38;5;66;03m#log the step\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 7\u001b[0m, in \u001b[0;36mcustom_policy\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      4\u001b[0m player_total \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#decide to stand (0) if total is 19 or above, otherwise request card (1)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m player_total \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m#stand\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "#play 5 games using the custom policy\n",
    "results = play_blackjack(env, custom_policy, games=5)\n",
    "\n",
    "#analyse the game results\n",
    "analyse_results(results)\n",
    "\n",
    "#estimate Q using monte carlo prediction\n",
    "Q = monte_carlo_prediction(env, custom_policy, episodes=5000)\n",
    "print('Action-value function estimated.')\n",
    "\n",
    "#find optimal policy using monte carlo control\n",
    "optimal_policy, optimal_Q = monte_carlo_control(env, episodes=10000, alpha=0.1)\n",
    "print('Optimal policy obtained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01310f4e-0291-4c28-9c89-173f018e0530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1f39a-c0e7-44ca-906f-401a1afac2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
